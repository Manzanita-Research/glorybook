---
phase: 01-sync-layer-hardening
plan: 02
type: execute
wave: 2
depends_on:
  - 01
files_modified:
  - src/server/deadsync-server.ts
autonomous: true
requirements:
  - SYNC-01
  - SYNC-03
must_haves:
  truths:
    - "Server enables hibernation and restores state from storage on wake-up"
    - "Setlist songs are stored as individual keys, not one blob, each under 128 KiB"
    - "Server waits 30 seconds before promoting a new leader when the current leader disconnects"
    - "If the original leader reconnects within the grace period, they reclaim leadership automatically"
    - "Promotion goes to the first follower who joined the session"
    - "Server handles all client message types without errors"
  artifacts:
    - path: "src/server/deadsync-server.ts"
      provides: "Hardened PartyKit server with hibernation, sharded storage, leader grace period"
      min_lines: 200
  key_links:
    - from: "src/server/deadsync-server.ts"
      to: "src/shared/protocol.ts"
      via: "imports message types and data model"
      pattern: "import.*protocol"
    - from: "src/server/deadsync-server.ts"
      to: "PartyKit storage API"
      via: "sharded storage keys for songs"
      pattern: 'storage\.(put|get|list|delete)'
    - from: "src/server/deadsync-server.ts"
      to: "PartyKit alarm API"
      via: "setAlarm for leader grace period"
      pattern: "storage\\.setAlarm"
---

<objective>
Harden the PartyKit server: enable hibernation, shard storage to handle 20+ song setlists within Cloudflare's 128 KiB per-value limit, and add a 30-second leader grace period using PartyKit alarms.

Purpose: The server is the authority for all session state. These three fixes address the known bugs: no hibernation support (SYNC-03), 128 KiB storage limit (SYNC-03), and eager leader promotion (SYNC-01).

Output: Rewritten `src/server/deadsync-server.ts` that compiles against the new protocol types.
</objective>

<execution_context>
@/Users/jem/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jem/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-sync-layer-hardening/01-CONTEXT.md
@.planning/phases/01-sync-layer-hardening/01-RESEARCH.md
@.planning/phases/01-sync-layer-hardening/01-01-SUMMARY.md
@src/shared/protocol.ts
@src/server/deadsync-server.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite server with hibernation and sharded storage</name>
  <files>src/server/deadsync-server.ts</files>
  <action>
Rewrite `src/server/deadsync-server.ts` to use the new protocol types from Plan 01 and fix the three server-side bugs. Preserve the existing handler structure (handleJoin, handleSetSong, handleBrowse, handleGoLive, handleSetSetlist, handleTransferLead, onRequest) but fix the internals.

**1. Enable hibernation:**
```typescript
options: Party.ServerOptions = { hibernate: true };
```

**2. Sharded storage — split the monolithic "session" blob into separate keys:**
- `"meta"` key: `{ sessionCode: string, liveIndex: number, leaderId: string | null }`
- `"setlist-info"` key: `{ id: string, name: string, songCount: number }`
- `"song:{index}"` keys: individual Song objects (one per song)
- `"users"` key: not stored — users are transient (connections are the source of truth). Only persist meta and setlist.
- `"disconnectedLeader"` key: `{ id: string, name: string, disconnectedAt: number }` — for grace period reclaim

**3. Update `onStart` for hibernation wake-up:**
Load only the meta key in onStart (keep it lean for hibernation). Load songs lazily when building state for clients.

```typescript
async onStart() {
  const meta = await this.room.storage.get<Meta>("meta");
  if (meta) {
    this.sessionCode = meta.sessionCode;
    this.liveIndex = meta.liveIndex;
    this.leaderId = meta.leaderId;
  } else {
    this.sessionCode = generateSessionCode();
  }
  const setlistInfo = await this.room.storage.get<SetlistInfo>("setlist-info");
  if (setlistInfo) {
    this.setlistInfo = setlistInfo;
  }
}
```

**4. Lazy song loading:**
Create a `getFullSetlist()` method that reads all `song:*` keys and assembles the Setlist object. Call this when building state for `onConnect` and `request-state`.

```typescript
async getFullSetlist(): Promise<Setlist> {
  const songs: Song[] = [];
  const entries = await this.room.storage.list<Song>({ prefix: "song:" });
  // entries is a Map — sort by key to maintain order
  const sorted = [...entries.entries()].sort(([a], [b]) => {
    const numA = parseInt(a.split(":")[1]);
    const numB = parseInt(b.split(":")[1]);
    return numA - numB;
  });
  for (const [, song] of sorted) {
    songs.push(song);
  }
  return { id: this.setlistInfo.id, name: this.setlistInfo.name, songs };
}
```

**5. Sharded persistence:**
Create `persistMeta()` and `persistSetlist(setlist)` methods:
- `persistMeta()` saves the meta key only
- `persistSetlist(setlist)` saves setlist-info + individual song keys (delete old songs first, then write new ones)

**6. Leader grace period with alarm:**

In `onClose`, when the leader disconnects:
- Store disconnected leader info: `this.room.storage.put("disconnectedLeader", { id, name, disconnectedAt: Date.now() })`
- Set alarm for 30 seconds: `this.room.storage.setAlarm(Date.now() + 30_000)`
- Broadcast `leader-disconnected` message with `graceSeconds: 30`
- Do NOT immediately promote — the leader might reconnect
- Remove the user from the in-memory users Map and broadcast user-left

In `onAlarm`:
- Check if `disconnectedLeader` key still exists (hasn't been cleared by a reconnect)
- If still exists: promote the first user by joinedAt order, broadcast `leader-changed`, clear `disconnectedLeader`
- If cleared: do nothing (leader already reconnected)

In `handleJoin`, when a user joins:
- Check `disconnectedLeader` storage key
- If the joining user's name matches the disconnected leader's name: cancel the alarm (`this.room.storage.deleteAlarm()`), clear `disconnectedLeader`, give them leader role regardless of what they requested, broadcast `leader-changed`
- Per CONTEXT.md: "If original leader reconnects (even after promotion), they automatically reclaim leadership"

**7. Promotion order:**
Per CONTEXT.md: "Promotion goes to first follower who joined the session (predictable)." Use SessionUser.joinedAt to determine order. When promoting in `onAlarm`, find the user with the earliest joinedAt.

**8. Update `onConnect`:**
Send full state (including full setlist from `getFullSetlist()`) to the new connection. The server already does this — just update to use the new sharded storage reader.

**9. Keep the HTTP `onRequest` endpoint** for session info (QR code target). Update to use sharded storage.

**Important:** Import types from the rewritten protocol.ts (Plan 01). Use `Party.Room` API. All handler methods should be async where they use storage.
  </action>
  <verify>
    <automated>cd /Users/jem/code/manzanita-research/glorybook && npx tsc --noEmit src/server/deadsync-server.ts</automated>
  </verify>
  <done>Server compiles against new protocol types. Hibernation enabled. Storage uses sharded keys (meta, setlist-info, song:N). Leader grace period uses setAlarm with 30-second delay. onAlarm promotes by joinedAt order. Reconnecting leader reclaims via handleJoin check.</done>
</task>

<task type="auto">
  <name>Task 2: Seed default setlist into storage on first start</name>
  <files>src/server/deadsync-server.ts</files>
  <action>
In `onStart`, after checking for existing stored state: if no setlist-info exists in storage, seed from DEFAULT_SETLIST.

```typescript
if (!setlistInfo) {
  // First start — seed default setlist
  this.setlistInfo = {
    id: DEFAULT_SETLIST.id,
    name: DEFAULT_SETLIST.name,
    songCount: DEFAULT_SETLIST.songs.length,
  };
  await this.room.storage.put("setlist-info", this.setlistInfo);
  for (let i = 0; i < DEFAULT_SETLIST.songs.length; i++) {
    await this.room.storage.put(`song:${i}`, DEFAULT_SETLIST.songs[i]);
  }
  await this.persistMeta();
}
```

This ensures a new room immediately has demo data available. The `handleSetSetlist` method should also use the sharded pattern (delete old songs, write new ones).
  </action>
  <verify>
    <automated>cd /Users/jem/code/manzanita-research/glorybook && npx tsc --noEmit src/server/deadsync-server.ts</automated>
  </verify>
  <done>Server seeds default setlist into sharded storage on first start. handleSetSetlist also uses sharded storage. Server compiles cleanly.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit src/server/deadsync-server.ts` passes
- Server class has `options: Party.ServerOptions = { hibernate: true }`
- Storage uses separate keys: "meta", "setlist-info", "song:0", "song:1", etc.
- `onClose` sets a 30-second alarm instead of immediately promoting
- `onAlarm` promotes the user with earliest joinedAt
- `handleJoin` checks for disconnectedLeader and reclaims leadership
- Default setlist is seeded into sharded storage on first start
- `handleSetSetlist` writes songs as individual keys
</verification>

<success_criteria>
Server compiles. Hibernation is enabled. Storage is sharded so no single value exceeds 128 KiB. Leader grace period uses a 30-second alarm. Reconnecting leader reclaims automatically. First follower by join order gets promoted.
</success_criteria>

<output>
After completion, create `.planning/phases/01-sync-layer-hardening/01-02-SUMMARY.md`
</output>
